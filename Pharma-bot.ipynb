{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92708dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from all_drug_data.json...\n",
      "Processing raw data into documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entries: 100%|██████████| 10000/10000 [00:00<00:00, 147059.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 13833 documents.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 48614 chunks.\n",
      "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kt/mk2ydbkd27q9x63d_1h4cb4c0000gn/T/ipykernel_7633/682425697.py:55: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and saving the FAISS vector store...\n",
      "✅ Vector store created and saved locally as 'faiss_drug_index'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_and_save_vector_store(json_path=\"all_drug_data.json\", save_path=\"faiss_drug_index\"):\n",
    "    \"\"\"Loads data, creates chunks, embeds them, and saves to a FAISS vector store.\"\"\"\n",
    "\n",
    "    # 1. Load your collected data\n",
    "    print(f\"Loading data from {json_path}...\")\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    all_docs = []\n",
    "    print(\"Processing raw data into documents...\")\n",
    "    # 2. Process raw JSON into LangChain Document objects\n",
    "    for entry in tqdm(data, desc=\"Processing entries\"):\n",
    "        # We only process entries that have some useful information\n",
    "        if not entry:\n",
    "            continue\n",
    "\n",
    "        brand_name_list = entry.get(\"openfda\", {}).get(\"brand_name\", [\"Unknown Brand\"])\n",
    "        generic_name_list = entry.get(\"openfda\", {}).get(\"generic_name\", [\"Unknown Generic\"])\n",
    "        brand_name = brand_name_list[0] if brand_name_list else \"Unknown Brand\"\n",
    "        generic_name = generic_name_list[0] if generic_name_list else \"Unknown Generic\"\n",
    "\n",
    "        sections_to_process = {\n",
    "            \"drug_interactions\": \"Drug Interactions\",\n",
    "            \"adverse_reactions\": \"Adverse Reactions\",\n",
    "            \"contraindications\": \"Contraindications\",\n",
    "            \"description\": \"Description\"\n",
    "        }\n",
    "\n",
    "        for key, section_name in sections_to_process.items():\n",
    "            # Extract text, which is often nested in a list\n",
    "            text_list = entry.get(key)\n",
    "            if text_list and isinstance(text_list, list) and text_list[0].strip():\n",
    "                text = text_list[0]\n",
    "                metadata = {\"brand_name\": brand_name, \"generic_name\": generic_name, \"section\": section_name}\n",
    "                doc = Document(page_content=text, metadata=metadata)\n",
    "                all_docs.append(doc)\n",
    "\n",
    "    print(f\"Created {len(all_docs)} documents.\")\n",
    "\n",
    "    # 3. Split the documents into smaller chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    split_docs = text_splitter.split_documents(all_docs)\n",
    "    print(f\"Split into {len(split_docs)} chunks.\")\n",
    "\n",
    "    # 4. Initialize the embedding model\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    print(f\"Loading embedding model: {model_name}...\")\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "    # 5. Create the FAISS vector store and save it\n",
    "    print(\"Creating and saving the FAISS vector store...\")\n",
    "    vector_store = FAISS.from_documents(split_docs, embeddings)\n",
    "    vector_store.save_local(save_path)\n",
    "    print(f\"✅ Vector store created and saved locally as '{save_path}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_and_save_vector_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f049e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
